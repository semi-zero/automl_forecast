{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cebe1c1-6fbb-4dca-9183-8b93953518e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logger 세팅\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from loggers import logger\n",
    "\n",
    "from process import input_data, preprocess\n",
    "\n",
    "def set_logger(log_name):\n",
    "    log_obj = logger.AutoMLLog(log_name)\n",
    "    log_obj.set_handler('automl_process')\n",
    "    log_obj.set_formats()\n",
    "    auto_logger = log_obj.addOn()\n",
    "    \n",
    "    auto_logger.info('logger 세팅')\n",
    "    \n",
    "log_name = 'practice'\n",
    "set_logger(log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8648d4-e0d3-46b9-80cd-a8f6894e6384",
   "metadata": {},
   "source": [
    "# **1. 데이터 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5118dc-5fec-4d17-946b-0085b5d164f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "class Data_load:\n",
    "    def __init__(self, path, log_name):\n",
    "        self.path = path #데이터 위치 경로 입력\n",
    "        self.logger = logging.getLogger(log_name)\n",
    "\n",
    "    def read_data(self):\n",
    "\n",
    "        self.logger.info('csv 데이터 불러오기')\n",
    "        self.logger.info(f'{self.path}')\n",
    "        \n",
    "        \n",
    "        #코드 수정해야 함 \n",
    "        try:\n",
    "            df = pd.read_csv(self.path)\n",
    "        except:\n",
    "            try:\n",
    "                df = pd.read_csv(self.path, encoding='cp949')\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_csv(self.path, encoding='utf-8')\n",
    "                except:\n",
    "                    self.logger.info('데이터 포맷을 맞춰주세요')\n",
    "        \n",
    "        self.logger.info('변수 분리 시작')\n",
    "        try:\n",
    "            var_list = df.columns.tolist() #전체 변수리스트 추출\n",
    "            num_var = df.select_dtypes(include='float').columns.tolist() + df.select_dtypes(include='int').columns.tolist() #수치형 변수 추출\n",
    "            obj_var = [x for x in df.columns if x not in num_var] #문자형 변수 추출\n",
    "        \n",
    "        except: \n",
    "            self.logger.error('csv 데이터 불러오기를 실패했습니다')\n",
    "        \n",
    "        df = self.reduce_mem_usage(df)\n",
    "        \n",
    "        return df, var_list, num_var, obj_var\n",
    "    \n",
    "    #데이터 메모리 줄이기\n",
    "    def reduce_mem_usage(self, df):\n",
    "        \"\"\" \n",
    "        iterate through all the columns of a dataframe and \n",
    "        modify the data type to reduce memory usage.        \n",
    "        \"\"\"\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        self.logger.info(f'데이터 구성: {df.shape[0]} 행, {df.shape[1]}열')\n",
    "        self.logger.info(f'Memory usage of dataframe is {start_mem:.2f}MB')\n",
    "    \n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "        \n",
    "            if col_type != object:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max <\\\n",
    "                    np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max <\\\n",
    "                    np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max <\\\n",
    "                    np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max <\\\n",
    "                    np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                elif str(col_type)[:5] == 'float':\n",
    "                    if c_min > np.finfo(np.float16).min and c_max <\\\n",
    "                    np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max <\\\n",
    "                    np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                df[col] = df[col].astype('category')\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        self.logger.info(f'Memory usage after optimization is: {end_mem:.2f}MB')\n",
    "        self.logger.info(f'Decreased by {100*((start_mem - end_mem)/start_mem):.1f}%')\n",
    "    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2997d363-b715-401b-9c07-7d2bc570b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "csv 데이터 불러오기\n",
      "storage/demand_forecast_dataset.csv\n",
      "변수 분리 시작\n",
      "데이터 구성: 115050 행, 39열\n",
      "Memory usage of dataframe is 34.23MB\n",
      "Memory usage after optimization is: 9.46MB\n",
      "Decreased by 72.4%\n"
     ]
    }
   ],
   "source": [
    "data, var_list, num_var, obj_var = input_data.Data_load('storage/demand_forecast_dataset.csv', log_name).read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabac890-6ffd-4dd0-a1ba-cc0697176957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sale_dy</th>\n",
       "      <th>str_cd</th>\n",
       "      <th>str_nm</th>\n",
       "      <th>l1_cd</th>\n",
       "      <th>l1_nm</th>\n",
       "      <th>prod_cd</th>\n",
       "      <th>prod_nm</th>\n",
       "      <th>sale_qty</th>\n",
       "      <th>dc_stk_qty</th>\n",
       "      <th>dyoff_dy</th>\n",
       "      <th>...</th>\n",
       "      <th>outsd_forn_visit</th>\n",
       "      <th>tot_visit</th>\n",
       "      <th>ride_pasgr_num</th>\n",
       "      <th>alight_pasgr_num</th>\n",
       "      <th>tot_pop</th>\n",
       "      <th>male_pop</th>\n",
       "      <th>female_pop</th>\n",
       "      <th>search_cnt</th>\n",
       "      <th>click_cnt</th>\n",
       "      <th>disa_subs_dur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>118614</td>\n",
       "      <td>172832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.468750</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38373</td>\n",
       "      <td>97771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>42375</td>\n",
       "      <td>100251</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.406250</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-22</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>41744</td>\n",
       "      <td>99396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.531250</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-29</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38386</td>\n",
       "      <td>97200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>107955</td>\n",
       "      <td>156920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-02-12</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>37786</td>\n",
       "      <td>97009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.640625</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29970</td>\n",
       "      <td>80321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.468750</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-02-26</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49538</td>\n",
       "      <td>109227</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.281250</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>110438</td>\n",
       "      <td>G속초조양</td>\n",
       "      <td>106</td>\n",
       "      <td>가공채소/계란</td>\n",
       "      <td>46145000</td>\n",
       "      <td>CJ)양념이잘배는찌개두부300G</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>30658</td>\n",
       "      <td>90606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.281250</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sale_dy  str_cd str_nm  l1_cd    l1_nm   prod_cd            prod_nm  \\\n",
       "0  2019-01-01  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "1  2019-01-08  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "2  2019-01-15  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "3  2019-01-22  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "4  2019-01-29  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "5  2019-02-05  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "6  2019-02-12  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "7  2019-02-19  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "8  2019-02-26  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "9  2019-03-05  110438  G속초조양    106  가공채소/계란  46145000  CJ)양념이잘배는찌개두부300G   \n",
       "\n",
       "   sale_qty  dc_stk_qty  dyoff_dy  ...  outsd_forn_visit  tot_visit  \\\n",
       "0        10           0         0  ...            118614     172832   \n",
       "1         5           0         0  ...             38373      97771   \n",
       "2        31           0         0  ...             42375     100251   \n",
       "3        22           0         0  ...             41744      99396   \n",
       "4         3           0         0  ...             38386      97200   \n",
       "5        18           0         0  ...            107955     156920   \n",
       "6        12           0         0  ...             37786      97009   \n",
       "7         9           0         0  ...             29970      80321   \n",
       "8        24           0         0  ...             49538     109227   \n",
       "9        10           0         0  ...             30658      90606   \n",
       "\n",
       "   ride_pasgr_num alight_pasgr_num  tot_pop  male_pop  female_pop  search_cnt  \\\n",
       "0               0                0        0         0           0   26.468750   \n",
       "1               0                0        0         0           0   38.250000   \n",
       "2               0                0        0         0           0   29.406250   \n",
       "3               0                0        0         0           0   23.531250   \n",
       "4               0                0        0         0           0   32.343750   \n",
       "5               0                0        0         0           0    0.000000   \n",
       "6               0                0        0         0           0   17.640625   \n",
       "7               0                0        0         0           0   26.468750   \n",
       "8               0                0        0         0           0   35.281250   \n",
       "9               0                0        0         0           0   35.281250   \n",
       "\n",
       "   click_cnt  disa_subs_dur  \n",
       "0         29              0  \n",
       "1         41              0  \n",
       "2         35              0  \n",
       "3         42              0  \n",
       "4         34              0  \n",
       "5         19              0  \n",
       "6         44              0  \n",
       "7         48              0  \n",
       "8         35              0  \n",
       "9         35              0  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7160a0-c2ac-4d70-b2a0-e7a20071ec7d",
   "metadata": {},
   "source": [
    "# **2. 데이터 전처리 하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd386d6c-e1a0-4f80-a9d9-079fff1ca325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "#전처리 class\n",
    "class Preprocessing:\n",
    "    def __init__(self, log_name, data, var_list, num_var, obj_var, target_var, date_var, store_list, unit, anomaly_per=10):\n",
    "        self.df = data                                     # 데이터\n",
    "        self.var_list = var_list                           # 전체 변수 리스트\n",
    "        self.num_var = num_var                             # 수치형 변수 리스트\n",
    "        self.obj_var = obj_var                             # 문자형 변수 리스트\n",
    "        self.target_var = target_var                       # 타겟 변수\n",
    "        self.date_var = date_var                           # 시간 변수\n",
    "        self.store_list = store_list                       # 지점(상품 변수)\n",
    "        self.unit = unit                                   # 시간 단위\n",
    "        \n",
    "        self._anomaly_ratio = int(anomaly_per)             # 지정 결측 범위\n",
    "        self._anomaly_percentage = int(anomaly_per) / 100  # 지정 결측 범위\n",
    "        \n",
    "        self.logger = logging.getLogger(log_name)\n",
    "        \n",
    "        #결측치 처리 먼저 진행\n",
    "        self.df = self.na_preprocess(self.df, self._anomaly_ratio)\n",
    "        \n",
    "        self.df, self.tds_df = self.tds_preprocess(self.df, self.target_var, self.date_var, self.store_list)\n",
    "        \n",
    "        # 표준화\n",
    "        self.df = self.standardize(self.df, self.num_var)\n",
    "        \n",
    "        # 라벨 인코딩\n",
    "        self.df = self.label_encoder(self.df, self.obj_var)\n",
    "        \n",
    "        # 일반 전처리 완료\n",
    "        self.df = self.get_df()\n",
    "        \n",
    "        # 시계열 전처리 수행\n",
    "        self.df = self.ts_preprocess(self.df, self.target_var, self.date_var, self.store_list, self.unit)\n",
    "    \n",
    "    \n",
    "        # 결측치 확인 및 처리\n",
    "    def na_preprocess(self, df, anomaly_per):\n",
    "        \n",
    "        self.logger.info('결측치 처리')\n",
    "        \n",
    "        try:\n",
    "            #Column별 결측치 n% 이상 있을 경우 제외\n",
    "            remove_v1 = round(df.isnull().sum() / len(df)*100, 2)\n",
    "            tmp_df = df[remove_v1[remove_v1 < anomaly_per].index]\n",
    "        \n",
    "            #Row별 결측치 n% 이상 있을 경우 제외\n",
    "            idx1 = len(tmp_df.columns) * 0.7\n",
    "        \n",
    "        except:\n",
    "            self.logger.exception('결측치 처리에 문제가 발생하였습니다')\n",
    "        \n",
    "        self.logger.info(f'결측치 처리 이후 데이터 구성: {df.shape[0]} 행, {df.shape[1]}열')                  \n",
    "        \n",
    "        return tmp_df.dropna(thresh=idx1, axis=0)\n",
    "    \n",
    "  \n",
    "    def tds_preprocess(self, df, target_var, date_var, store_list):\n",
    "        #target, date, store\n",
    "        self.logger.info('전처리를 위한 target, date, store 분리')\n",
    "        \n",
    "        #식별 변수가 있을 수도 있고 없을 수도 있다(0119)\n",
    "        \n",
    "        try:\n",
    "            tds_df = df[[target_var, date_var]+ store_list]\n",
    "            df = df.drop([target_var, date_var]+ store_list, axis=1)\n",
    "            \n",
    "            if date_var in self.num_var:\n",
    "                self.num_var.remove(date_var)    \n",
    "            else : self.obj_var.remove(date_var)\n",
    "            \n",
    "            for store_var in store_list:\n",
    "                if store_var in self.num_var:\n",
    "                    self.num_var.remove(store_var)    \n",
    "                else : self.obj_var.remove(store_var)\n",
    "\n",
    "            if target_var in self.num_var:\n",
    "                self.num_var.remove(target_var)    \n",
    "            else : self.obj_var.remove(target_var)\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            self.logger.exception(' target, date, store 분리 처리에 문제가 발생하였습니다')\n",
    "            \n",
    "        return df, tds_df\n",
    "        \n",
    "    \n",
    "#     # 이상치 제거 절차 삭제(230119)\n",
    "        \n",
    "    \n",
    "    #정규화\n",
    "    def standardize(self, df, num_var):\n",
    "                                  \n",
    "        self.logger.info('정규화 진행')\n",
    "        try:        \n",
    "            if num_var:\n",
    "                num_data = df.loc[:, num_var]\n",
    "                non_num_data = df.drop(set(num_var), axis=1)\n",
    "\n",
    "                #표준화\n",
    "                std_scaler = StandardScaler()\n",
    "                fitted = std_scaler.fit(num_data)\n",
    "                output = std_scaler.transform(num_data)\n",
    "                num_data = pd.DataFrame(output, columns = num_data.columns, index=list(num_data.index.values))\n",
    "\n",
    "                tmp_df = pd.concat([non_num_data, num_data], axis=1)\n",
    "            else:\n",
    "                tmp_df = df\n",
    "        except:\n",
    "            self.logger.exception('정규화 진행 중에 문제가 발생하였습니다')                                      \n",
    "                                  \n",
    "        return tmp_df\n",
    "        \n",
    "    \n",
    "    #문자형 변수를 수치형으로 변환\n",
    "    def label_encoder(self, df, obj_var):\n",
    "                                  \n",
    "        self.logger.info('라벨 인코딩 진행')\n",
    "        try:                              \n",
    "            if obj_var:\n",
    "                obj_data = df.loc[:, obj_var]\n",
    "                non_obj_data = df.drop(set(obj_var), axis=1)\n",
    "\n",
    "                #인코딩\n",
    "                lbl_en = LabelEncoder()\n",
    "                lbl_en = defaultdict(LabelEncoder)\n",
    "                obj_data = obj_data.apply(lambda x:lbl_en[x.name].fit_transform(x))\n",
    "            \n",
    "                #라벨 인코딩 저장    \n",
    "                pickle.dump(lbl_en, open('storage/label_encoder.sav', 'wb'))\n",
    "                \n",
    "            \n",
    "                tmp_df = pd.concat([obj_data, non_obj_data], axis=1)\n",
    "                \n",
    "            else:\n",
    "                tmp_df = df\n",
    "                                  \n",
    "        except:\n",
    "            self.logger.exception('수치형 변환 중에 문제가 발생하였습니다')                                      \n",
    "                                 \n",
    "        return tmp_df\n",
    "    \n",
    "    \n",
    "    def get_df(self):\n",
    "        \n",
    "        self.df = pd.concat([self.df, self.tds_df], axis=1)\n",
    "\n",
    "        self.logger.info('전처리 완료')\n",
    "        self.logger.info('\\n')\n",
    "        self.logger.info(self.df.head())\n",
    "        \n",
    "        \n",
    "        return self.df\n",
    "\n",
    "    \n",
    "    def ts_preprocess(self, data, target_var, date_var, store_list, unit):\n",
    "        \n",
    "        self.logger.info('시계열용 전처리 진행')\n",
    "        try:    \n",
    "            #store_var type이 str이어야 함\n",
    "            for store_var in store_list:\n",
    "                data[store_var] = data[store_var].astype(str)\n",
    "            \n",
    "            try:\n",
    "                data[date_var] = pd.to_datetime(data[date_var],infer_datetime_format = True, utc = True).astype('datetime64[ns]')\n",
    "            \n",
    "            except:\n",
    "                self.logger.info('날짜 변수를 확인해주세요')\n",
    "            #except:\n",
    "            #    try:\n",
    "            #        data[date_var] = data[date_var].apply(lambda x : datetime.strptime(str(x), '%Y%m%d'))\n",
    "            #    except:\n",
    "            #        self.logger.info('날짜 변수를 확인해주세요')\n",
    "            # => 날짜 양식에 따라 계속 확장해 나가야 함\n",
    "            \n",
    "            #store_list가 하나일 때\n",
    "            if len(store_list) == 1 :\n",
    "                store_list = ['dummy'] + store_list\n",
    "                data['dummy'] = 'dummy'\n",
    "                \n",
    "            df = pd.DataFrame()\n",
    "            for store_var_0, store_var_1 in data.drop_duplicates(store_list)[store_list].values:\n",
    "                tmp_df = data.loc[(data[store_list[0]]==store_var_0)&(data[store_list[1]]==store_var_1), :]\n",
    "                tmp_df = tmp_df.sort_values(date_var).reset_index(drop=True)\n",
    "                tmp_df['time_idx'] = tmp_df.index\n",
    "                df = pd.concat([df, tmp_df], axis=0)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "            # add additional features\n",
    "            if unit == 'day':\n",
    "                df[unit] = df[date_var].dt.day.astype(str).astype(\"category\")\n",
    "            elif unit == 'week':\n",
    "                df[unit] = df[date_var].dt.isocalendar().week.astype(str).astype(\"category\")  # categories have be strings\n",
    "            elif unit == 'month':\n",
    "                df[unit] = df[date_var].dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "            \n",
    "            self.logger.info('시계열 전처리 후 df')\n",
    "            self.logger.info(df.head())\n",
    "        except:\n",
    "            self.logger.exception('시계열용 전처리 진행 중에 문제가 발생하였습니다')     \n",
    "        \n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a9843c-347e-4038-8edc-9910b52674fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "결측치 처리\n",
      "결측치 처리 이후 데이터 구성: 115050 행, 39열\n",
      "전처리를 위한 target, date, store 분리\n",
      "정규화 진행\n",
      "라벨 인코딩 진행\n",
      "전처리 완료\n",
      "\n",
      "\n",
      "   str_nm  l1_nm  prod_nm  evt_dur_div  avg_tmpr_val  search_cnt     l1_cd  \\\n",
      "0       0      0        0            4     -1.184225    0.228434 -1.160689   \n",
      "1       0      0        0            4     -1.001111    0.897404 -1.160689   \n",
      "2       0      0        0            4     -1.001111    0.395233 -1.160689   \n",
      "3       0      0        0            4     -0.783086    0.061635 -1.160689   \n",
      "4       0      0        0            4     -0.896505    0.562032 -1.160689   \n",
      "\n",
      "   dc_stk_qty  dyoff_dy  holidy_cd  ...  alight_pasgr_num   tot_pop  male_pop  \\\n",
      "0   -0.165029       0.0   2.612763  ...         -1.249877 -1.789319  -1.74308   \n",
      "1   -0.165029       0.0  -0.382737  ...         -1.249877 -1.789319  -1.74308   \n",
      "2   -0.165029       0.0  -0.382737  ...         -1.249877 -1.789319  -1.74308   \n",
      "3   -0.165029       0.0  -0.382737  ...         -1.249877 -1.789319  -1.74308   \n",
      "4   -0.165029       0.0  -0.382737  ...         -1.249877 -1.789319  -1.74308   \n",
      "\n",
      "   female_pop  click_cnt  disa_subs_dur  sale_qty     sale_dy  str_cd  \\\n",
      "0   -1.823571  -1.372743      -0.396059        10  2019-01-01  110438   \n",
      "1   -1.823571  -0.702655      -0.396059         5  2019-01-08  110438   \n",
      "2   -1.823571  -1.037699      -0.396059        31  2019-01-15  110438   \n",
      "3   -1.823571  -0.646815      -0.396059        22  2019-01-22  110438   \n",
      "4   -1.823571  -1.093540      -0.396059         3  2019-01-29  110438   \n",
      "\n",
      "    prod_cd  \n",
      "0  46145000  \n",
      "1  46145000  \n",
      "2  46145000  \n",
      "3  46145000  \n",
      "4  46145000  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "시계열용 전처리 진행\n",
      "시계열 전처리 후 df\n",
      "   str_nm  l1_nm  prod_nm  evt_dur_div  avg_tmpr_val  search_cnt     l1_cd  \\\n",
      "0       0      0        0            4     -1.184225    0.228434 -1.160689   \n",
      "1       0      0        0            4     -1.140628   -0.105164 -1.160689   \n",
      "2       0      0        0            4     -1.035980    0.562032 -1.160689   \n",
      "3       0      0        0            4     -0.870279    0.228434 -1.160689   \n",
      "4       0      0        0            4     -0.913875    0.061635 -1.160689   \n",
      "\n",
      "   dc_stk_qty  dyoff_dy  holidy_cd  ...  male_pop  female_pop  click_cnt  \\\n",
      "0   -0.165029       0.0   2.612763  ...  -1.74308   -1.823571  -1.372743   \n",
      "1   -0.165029       0.0  -0.382737  ...  -1.74308   -1.823571  -0.926018   \n",
      "2   -0.165029       0.0  -0.382737  ...  -1.74308   -1.823571  -1.093540   \n",
      "3   -0.165029       0.0  -0.382737  ...  -1.74308   -1.823571  -1.205221   \n",
      "4   -0.165029       0.0  -0.382737  ...  -1.74308   -1.823571  -1.428583   \n",
      "\n",
      "   disa_subs_dur  sale_qty    sale_dy  str_cd   prod_cd  time_idx  day  \n",
      "0      -0.396059        10 2019-01-01  110438  46145000         0    1  \n",
      "1      -0.396059         9 2019-01-02  110438  46145000         1    2  \n",
      "2      -0.396059        11 2019-01-03  110438  46145000         2    3  \n",
      "3      -0.396059         3 2019-01-04  110438  46145000         3    4  \n",
      "4      -0.396059         3 2019-01-05  110438  46145000         4    5  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "df = preprocess.Preprocessing(log_name, data, var_list, num_var, obj_var, target_var='sale_qty', date_var= 'sale_dy', store_list=['str_cd','prod_cd'], unit='day').df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d611a864-ea14-4950-a080-e72b3998de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "\n",
    "#prophet\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "#from . import hpo\n",
    "\n",
    "\n",
    "class Modeling:\n",
    "    \n",
    "    def __init__(self, log_name, df, target_var, date_var, store_list, unit, predict_n, HPO):\n",
    "        self.df = df                                           # 데이터\n",
    "        self.target_var = target_var                           # 타겟 변수\n",
    "        self.date_var = date_var                               # 시간 변수\n",
    "        self.store_list = store_list                           # 지점(상품 변수)\n",
    "        self.unit = unit                                       # 시간 단위\n",
    "        self.predict_n = predict_n                             # 예측 기간\n",
    "        self.HPO = HPO                                         # 하이퍼파라미터 튜닝 여부\n",
    "        \n",
    "        \n",
    "        self.logger = logging.getLogger(log_name)\n",
    "       \n",
    "        \n",
    "        self.val_df = self.fb_fit_predict(self.df, self.target_var, self.date_var, self.store_list, self.unit, self.predict_n, self.HPO)\n",
    "            \n",
    "    def fb_fit_predict(self, df, target_var, date_var, store_list, unit, predict_n, HPO):\n",
    "        \n",
    "        self.logger.info('fbprophet 데이터 준비')\n",
    "        print(store_list)\n",
    "        \n",
    "        if len(store_list) == 1 :\n",
    "            store_list = ['dummy'] + store_list\n",
    "            df.loc[:, 'dummy'] = 'dummy'\n",
    "        \n",
    "        train_df = pd.DataFrame()\n",
    "        val_df = pd.DataFrame()\n",
    "        pred_df = pd.DataFrame()\n",
    "\n",
    "        for store_var_0, store_var_1 in df.drop_duplicates(store_list)[store_list].values:\n",
    "            fb_df = df.loc[(df[store_list[0]]==store_var_0)&(df[store_list[1]]==store_var_1), :]               \n",
    "            fb_df.loc[:, 'ds'] = fb_df[date_var]\n",
    "            fb_df.loc[:, 'y'] = fb_df[target_var]        \n",
    "            fb_df.loc[:, 'cap'] = np.max(fb_df[target_var].values)\n",
    "            fb_df.loc[:, 'floor'] = np.min(fb_df[target_var].values)\n",
    "\n",
    "            predict_size = predict_n\n",
    "            fb_train = fb_df.iloc[:-predict_size, :]\n",
    "            fb_var = fb_df.iloc[-predict_size:, :]\n",
    "\n",
    "            print(HPO)\n",
    "            if HPO :\n",
    "                self.logger.info('fb HPO 진행') \n",
    "                parameters = hpo.HyperOptimization(train = fb_train, valid = fb_var, model = 'fb').best_params\n",
    "                self.logger.info(f'fb HPO 진행 후 parameters: {parameters}')\n",
    "\n",
    "            else:\n",
    "                parameters = {'changepoint_prior_scale': 1.8, 'changepoint_range': 0.8, 'seasonality_prior_scale': 7.3, 'holidays_prior_scale': 6, 'seasonality_mode': 'multiplicative', 'weekly_seasonality': 5, 'yearly_seasonality': 18}\n",
    "\n",
    "            #validate 후 validate_df 생성\n",
    "            m = Prophet(**parameters)\n",
    "            m.fit(fb_train[['y','ds','cap','floor']], algorithm='Newton')\n",
    "            val_preds = m.predict(fb_var[['ds','cap','floor']])\n",
    "            val_preds = val_preds[['ds','yhat']]\n",
    "            val_real = fb_var[['y', date_var]]\n",
    "            val_preds_df = pd.merge(val_preds, val_real, left_on='ds', right_on=date_var, how='inner')\n",
    "            \n",
    "            #train\n",
    "            train = fb_train[['y','ds']]\n",
    "            train[store_list[0]] = store_var_0\n",
    "            train[store_list[1]] = store_var_1\n",
    "            train_df = pd.concat([train_df, train], axis=0)\n",
    "            \n",
    "            #valid\n",
    "            val_preds_df[store_list[0]] = store_var_0\n",
    "            val_preds_df[store_list[1]] = store_var_1\n",
    "            \n",
    "            val_df = pd.concat([val_df, val_preds_df], axis=0) \n",
    "            #predicat_date 생성 후 예측 predict_df생성\n",
    "\n",
    "            #m.fit(fb_df[['ds','cap','floor']])\n",
    "            last_date = fb_df[date_var].iloc[-1:].tolist()[0]\n",
    "            \n",
    "            if unit =='day':\n",
    "                predict_date = [last_date + timedelta(days=i) for i in range(1, predict_n+1)] #weeks, days 변경 가능\n",
    "            elif unit == 'week':\n",
    "                predict_date = [last_date + timedelta(days=7*i) for i in range(1, predict_n+1)] #weeks, days 변경 가능\n",
    "            elif unit == 'month':\n",
    "                predict_date = [last_date + timedelta(days=30*i) for i in range(1, predict_n+1)] #weeks, days 변경 가능\n",
    "            test_df = pd.DataFrame({'ds': predict_date})\n",
    "            test_df['cap'] = fb_df['cap'].values[0]\n",
    "            test_df['floor'] = fb_df['floor'].values[0]\n",
    "\n",
    "            preds = m.predict(test_df[['ds','cap','floor']])\n",
    "            preds = preds[['ds','yhat']]\n",
    "            preds[store_list[0]] = store_var_0\n",
    "            preds[store_list[1]] = store_var_1\n",
    "            pred_df = pd.concat([pred_df, preds], axis=0)\n",
    "\n",
    "        train_df.to_csv('train_df.csv', index=False)\n",
    "        val_df.to_csv('val_df.csv', index=False)\n",
    "        pred_df.to_csv('pred_df.csv', index=False)\n",
    "\n",
    "        return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5e897-f14d-4f05-bdd2-13cdcf5145c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
